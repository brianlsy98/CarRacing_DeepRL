
Introduction to Intelligent Systems


	    #################
	 *  # DQN : Trial 1 #	----------> almost Successful after 12 hours (3000 epi)
	    ###########################################################################
            # HyperParameters
            lr = 5e-4  # learning rate for gradient update 
            batchsize = 64  # batchsize for buffer sampling
            maxlength = 10000  # max number of tuples held by buffer
            tau = 100  # time steps for target update
            episodes = 3000  # number of episodes to run
            initialize = 500  # initial time steps before start updating
            eps = 1
            eps_minus = .0001

            gamma = .99  # discount
            hidden_dims=[128, 64, 32] # hidden dimensions
            ###########################################################################
            # setting before training
            obssize = 28    # 1081(lidar) + 2 -> 26 + 2
            actsize = 11    # input steering : 0, +-(0.3, 0.6, 0.9, 1.2, 1.5)
            optimizer = keras.optimizers.Adam(learning_rate=lr)

            Qprincipal = DQN(obssize, actsize, hidden_dims, optimizer)
            Qtarget = DQN(obssize, actsize, hidden_dims, optimizer)
            buffer = ReplayBuffer(maxlength)
            ###########################################################################
	    ||
	    >> Problem candidates :
		1. action size too large 11 -> 5	  # input_steering : 0, +-(0.5, 1.5)
		2. robot runs into the wall, but not done # obs certain point val < 1.5 : done = True
		3. eps decreasing rate too high		  # 0.0001 -> 0.00004
		4. hidden_dims not simple		  # [128, 64, 32] -> [64, 32]
		# 5. straight vs left-right		  # reward += 1 for straight


	    #################
	 *  # DQN : Trial 2 #	----------> unsuccessful
	    ###########################################################################
            # HyperParameters
            lr = 5e-4  # learning rate for gradient update 
            batchsize = 64  # batchsize for buffer sampling
            maxlength = 10000  # max number of tuples held by buffer
            tau = 100  # time steps for target update
            episodes = 5000  # number of episodes to run
            initialize = 500  # initial time steps before start updating
            eps = 1
            eps_minus = .00004

            gamma = .99  # discount
            hidden_dims=[64, 32] # hidden dimensions
            ###########################################################################
            # setting before training
            obssize = 28    # 1081(lidar) + 2 -> 26 + 2
            actsize = 5     # input steering : 0, +-(0.5, 1.5)
            optimizer = keras.optimizers.Adam(learning_rate=lr)

            Qprincipal = DQN(obssize, actsize, hidden_dims, optimizer)
            Qtarget = DQN(obssize, actsize, hidden_dims, optimizer)
            buffer = ReplayBuffer(maxlength)
            ###########################################################################
	    ||
	    >> Problem candidates :
		1. Terminated too easily		   # l, m, r < 0.9, 1.0, 0.9 -> 0.5, 0.8, 0.5
		2. DQN.save... -> DQN.qfunction.save


	    #################
	 *  # DQN : Trial 3 #	----------> unsuccessful ( only first corner )
	    ###########################################################################
            # HyperParameters
            lr = 5e-4  # learning rate for gradient update 
            batchsize = 64  # batchsize for buffer sampling
            maxlength = 10000  # max number of tuples held by buffer
            tau = 100  # time steps for target update
            episodes = 5000  # number of episodes to run
            initialize = 500  # initial time steps before start updating
            eps = 1
            eps_minus = .0001

            gamma = .99  # discount
            hidden_dims=[64, 32] # hidden dimensions
            ###########################################################################
            # setting before training
            obssize = 28    # 1081(lidar) + 2 -> 26 + 2
            actsize = 5     # input steering : 0, +-(0.5, 1.5)
            optimizer = keras.optimizers.Adam(learning_rate=lr)

            Qprincipal = DQN(obssize, actsize, hidden_dims, optimizer)
            Qtarget = DQN(obssize, actsize, hidden_dims, optimizer)
            buffer = ReplayBuffer(maxlength)
            ###########################################################################
	    ||
	    >> Problem candidates :
		1. Custom Termination		   	# obs -> np.min(obs_dim28) < 0.9
		2. save model only once			# save model every 200 ite
		3. model unsaved (call function)	# save model -> save weights
		4. hidden_dim too low			# [64 32] -> [128, 128, 128]
		5. action space too low			# 5 -> 7


	    #################
	 *  # DQN : Trial 4 #	----------> unsuccessful
	    ###########################################################################
            # HyperParameters
            lr = 5e-4  # learning rate for gradient update 
            batchsize = 64  # batchsize for buffer sampling
            maxlength = 10000  # max number of tuples held by buffer
            tau = 100  # time steps for target update
            episodes = 5000  # number of episodes to run
            initialize = 500  # initial time steps before start updating
            eps = 1
            eps_minus = .0001

            gamma = .99  # discount
            hidden_dims=[128, 128, 128] # hidden dimensions
            ###########################################################################
            # setting before training
            obssize = 28    # 1081(lidar) + 2 -> 26 + 2
            actsize = 7     # input steering : 0, +-(0.5, 1.0, 1.5)
            optimizer = keras.optimizers.Adam(learning_rate=lr)

            Qprincipal = DQN(obssize, actsize, hidden_dims, optimizer)
            Qtarget = DQN(obssize, actsize, hidden_dims, optimizer)
            buffer = ReplayBuffer(maxlength)
            ###########################################################################
	    ||
	    >> Problem candidates :
		1. back to trial 1... + reward += action[1] & reward -= 100




